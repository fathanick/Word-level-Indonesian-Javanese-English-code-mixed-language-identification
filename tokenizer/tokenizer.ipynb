{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.splitter import sentence_splitter\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import wordpunct_tokenize, word_tokenize, TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hati-hati', ',', 'ji-as', 'kk-ds', '.', 'baju', '.', '3x', '1,9', '12.45', '3-4', 'asda-asd', '!', 'satu', '+', 'dua']\n",
      "['hati-hati', ',', 'ji-as', 'kk-ds', '.', 'baju', '.', '3x', '1,9', '12.45', '3-4', 'asda-asd', '!', 'satu+dua']\n",
      "['hati', '-', 'hati', ',', 'ji', '-', 'as', 'kk', '-', 'ds', '.', 'baju', '.', '3x', '1', ',', '9', '12', '.', '45', '3', '-', '4', 'asda', '-', 'asd', '!', 'satu', '+', 'dua']\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "#sentence = \"~kk RT @GoddesofKebab lg😭😭😭 500rb😭🤲🏻 10juta bapak.aku internet! -ok temen\\\"ku refund-nya baik-baik baik/buruk nge-print (ads) (rumah kita) era 4.0 ngasih? tender :)) proyek, I'm Bob's I'll apps ovo...ke vendor abal2. #mantapkali https://towardsdatascience.com/pos-tagging-using-crfs-ea430c5fb78b\"\n",
    "#stc = ' Wis \\'lah...tidur satu\\' \"Gw dapet bedanya” ibu\" 500rb😭🤲🏻 “halah tinggal, ngeprint. sendiri apa bedanya” terburu-buru tok..hahaha 2.3 !ok iki..haha mari...\" .ki...,, ovo...ke vendor abal2. 10% 10juta siang/malam bapak2 &ok makan.minum bener² ibu\\\" -__-  (´･ω･`) he!! apa??? and -_- SIRAMPOG .mari...\\\" 14.07.2021 #instaNews Senin 14-07-2021 21/10/19 [telah] {terjadi} +angin kencang, di-print termasuk Sirampog. Kondisi (terkini) Sirampog, @100.000 aik-aik 12+ .. *info *tunggu* update dari ?kita ya! lur .. Do\\'a kan yg…  https://t.co/7Dl8UV2QZU'\n",
    "#stc = '\":v :V ;v lg😭😭😭 500rb😭🤲🏻  -_- -__- dapet bola aja jarang! Susah! Sekalinya dapet, pelanggaran! MAU KAMU APAA??\" “halah tinggal ngeprint sendiri apa bedanya” 🤣🤣🤣🤣 lemot \"@kecepoood: XL labil donlod munggah mudun 😩\"'\n",
    "#stc = '(´･ω･`) \\(^▿^)/ bener² baju\\'e I\\'m “halah may*ora df~~~ ~~~fsf tinggal, iki..haha !dfs &sdf !fsd mari. dsfg, ,sdf .ki ...,, ovo...ke (sendiri) (sdf aja) ngeprint. apa bedanya”  ~ff -fg sdf~ sfs- 4.3 iki..haha !dfs &sdf !fsd mari. dsfg, ,sdf .ki ...,, ovo...ke (sendiri) (sdf aja) $10 Rp10.000,00 11-23-22 12/12/32 15.00 @10.1000 #sdfs @sfsdf https://pynative.com/python-regex-compile/  -= STASIUN CITAYAM =- \" @sdfsd: beda :\\\") ;)) 🤣🤣🤣🤣'\n",
    "sentence = 'hati-hati, ji-as kk-ds. baju. 3x 1,9 12.45 3-4 asda-asd! satu+dua'\n",
    "print(sentence_splitter(sentence))\n",
    "print(word_tokenize(sentence))\n",
    "print(wordpunct_tokenize(sentence))\n",
    "#sentence.split()\n",
    "#stc.split()\n",
    "\n",
    "#PR: 1-3 not tokenized correctly, asda-asdasd, sddfs+sdfds,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_file = '../dataset/new-tagged-500.tsv'\n",
    "#source_file = '../dataset/filtered-tweet-500.xlsx'\n",
    "#target_file = '../dataset/new-tagged-300.tsv'\n",
    "#source_file = '../dataset/data-150322.xlsx'\n",
    "#target_file = '../dataset/new-tagged-1000.tsv'\n",
    "#source_file = '../dataset/data-210322.xlsx'\n",
    "target_file = '../raw dataset/all-tagged-test-data.tsv'\n",
    "source_file = '../raw dataset/data-050422-test.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_excel(source_file)\n",
    "tweets = all_data['tweet'].head(600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data = pd.read_excel(source_file, header=None)\n",
    "#tweets = all_data[0]\n",
    "#len(all_data)\n",
    "#tweets = all_data['tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "600"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indonesian dictionary\n",
    "id_dict_file = pd.read_csv('../dict/indonesian_words_final.csv')\n",
    "id_dict = id_dict_file.word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# named entity dictionary\n",
    "ne_dict = pd.read_csv('../dict/NE-list.txt', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Javanese dictionary\n",
    "jv_dict_file = pd.read_csv('../dict/unknown_words_java.csv')\n",
    "jv_dict = jv_dict_file.word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English dictionary\n",
    "en_dict_file = pd.read_csv('../dict/english_words_final.csv')\n",
    "en_dict = en_dict_file.word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mix ID-EN dictionary\n",
    "mix_id_en_file = pd.read_csv('../dict/mixed_words_final.csv')\n",
    "mix_id_en_dict = mix_id_en_file.word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mix JV-EN dictionary\n",
    "mix_jv_en_dict = pd.read_csv('../dict/jv-en.txt', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mix ID-JV dictionary\n",
    "mix_id_jv_dict = pd.read_csv('../dict/id-jv.txt', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagger(target_file, input_data):\n",
    "\ttweets = input_data\n",
    "\twith open(target_file, 'a', encoding='utf-8') as f:\n",
    "\t\tfor tw in tweets:\n",
    "\t\t\ttokens = sentence_splitter(tw)\n",
    "\t\t\tfor token in tokens:\n",
    "\t\t\t\ttlower = token.lower()\n",
    "\t\t\t\tif token.startswith('#') or token.startswith('@') or token.endswith('%') or token.isnumeric():\n",
    "\t\t\t\t\tf.write(token + '\\t' + 'O' + '\\n')\n",
    "\t\t\t\telif re.match(r'https?:\\/\\/.*[\\r\\n]*',token) or re.match(r'[\\W]', token):\n",
    "\t\t\t\t\tf.write(token + '\\t' + 'O' + '\\n')\n",
    "\t\t\t\telif tlower in ne_dict.values:\n",
    "\t\t\t\t\tf.write(token + '\\t' + 'NE' + '\\n')\n",
    "\t\t\t\telif tlower in id_dict.values:\n",
    "\t\t\t\t\tf.write(token + '\\t' + 'ID' + '\\n')\n",
    "\t\t\t\telif tlower in en_dict.values:\n",
    "\t\t\t\t\tf.write(token + '\\t' + 'EN' + '\\n')\n",
    "\t\t\t\telif tlower in jv_dict.values:\n",
    "\t\t\t\t\tf.write(token + '\\t' + 'JV' + '\\n')\n",
    "\t\t\t\telif tlower in mix_id_en_dict.values:\n",
    "\t\t\t\t\tf.write(token + '\\t' + 'MIX-ID-EN' + '\\n')\n",
    "\t\t\t\telif tlower in mix_id_jv_dict.values:\n",
    "\t\t\t\t\tf.write(token + '\\t' + 'MIX-ID-JV' + '\\n')\n",
    "\t\t\t\telif tlower in mix_jv_en_dict.values:\n",
    "\t\t\t\t\tf.write(token + '\\t' + 'MIX-JV-EN' + '\\n')\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tf.write(token + '\\t' + '\\n')\n",
    "\t\t\tf.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\u23f0' in position 0: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnicodeEncodeError\u001B[0m                        Traceback (most recent call last)",
      "Input \u001B[1;32mIn [17]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtagger\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtarget_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtarget_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtweets\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [16]\u001B[0m, in \u001B[0;36mtagger\u001B[1;34m(target_file, input_data)\u001B[0m\n\u001B[0;32m      9\u001B[0m \tf\u001B[38;5;241m.\u001B[39mwrite(token \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mO\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m re\u001B[38;5;241m.\u001B[39mmatch(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps?:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m/.*[\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mn]*\u001B[39m\u001B[38;5;124m'\u001B[39m,token) \u001B[38;5;129;01mor\u001B[39;00m re\u001B[38;5;241m.\u001B[39mmatch(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mW]\u001B[39m\u001B[38;5;124m'\u001B[39m, token):\n\u001B[1;32m---> 11\u001B[0m \t\u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;130;43;01m\\t\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mO\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m tlower \u001B[38;5;129;01min\u001B[39;00m ne_dict\u001B[38;5;241m.\u001B[39mvalues:\n\u001B[0;32m     13\u001B[0m \tf\u001B[38;5;241m.\u001B[39mwrite(token \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNE\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\encodings\\cp1252.py:19\u001B[0m, in \u001B[0;36mIncrementalEncoder.encode\u001B[1;34m(self, input, final)\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mencode\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, final\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m---> 19\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcodecs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcharmap_encode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\u001B[43mencoding_table\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mUnicodeEncodeError\u001B[0m: 'charmap' codec can't encode character '\\u23f0' in position 0: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "tagger(target_file=target_file, input_data=tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}