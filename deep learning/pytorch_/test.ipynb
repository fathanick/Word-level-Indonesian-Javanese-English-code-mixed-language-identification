{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helper.dataset_reader import read_tsv\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from torchtext.datasets import SequenceTaggingDataset\n",
    "from corpus import Corpus\n",
    "from spacy.lang.id import Indonesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 2889 sentences\n",
      "Val set: 723 sentences\n",
      "Test set: 1781 sentences\n"
     ]
    }
   ],
   "source": [
    "crp = Corpus(\n",
    "\tinput_path = '../../dataset/',\n",
    "\tmin_word_freq = 2,\n",
    "\tbatch_size = 32\n",
    ")\n",
    "print(f\"Train set: {len(crp.train_data)} sentences\")\n",
    "print(f\"Val set: {len(crp.val_data)} sentences\")\n",
    "print(f\"Test set: {len(crp.test_data)} sentences\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "\n",
    "  def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, lstm_layers,\n",
    "               emb_dropout, lstm_dropout, fc_dropout, word_pad_idx):\n",
    "    super().__init__()\n",
    "    self.embedding_dim = embedding_dim\n",
    "    # LAYER 1: Embedding\n",
    "    self.embedding = nn.Embedding(\n",
    "        num_embeddings=input_dim,\n",
    "        embedding_dim=embedding_dim,\n",
    "        padding_idx=word_pad_idx\n",
    "    )\n",
    "    self.emb_dropout = nn.Dropout(emb_dropout)\n",
    "    # LAYER 2: BiLSTM\n",
    "    self.lstm = nn.LSTM(\n",
    "        input_size=embedding_dim,\n",
    "        hidden_size=hidden_dim,\n",
    "        num_layers=lstm_layers,\n",
    "        bidirectional=True,\n",
    "        dropout=lstm_dropout if lstm_layers > 1 else 0\n",
    "    )\n",
    "    # LAYER 3: Fully-connected\n",
    "    self.fc_dropout = nn.Dropout(fc_dropout)\n",
    "    self.fc = nn.Linear(hidden_dim * 2, output_dim)  # times 2 for bidirectional\n",
    "\n",
    "  def forward(self, sentence):\n",
    "    # sentence = [sentence length, batch size]\n",
    "    # embedding_out = [sentence length, batch size, embedding dim]\n",
    "    embedding_out = self.emb_dropout(self.embedding(sentence))\n",
    "    # lstm_out = [sentence length, batch size, hidden dim * 2]\n",
    "    lstm_out, _ = self.lstm(embedding_out)\n",
    "    # ner_out = [sentence length, batch size, output dim]\n",
    "    lid_out = self.fc(self.fc_dropout(lstm_out))\n",
    "    return lid_out\n",
    "\n",
    "  def init_weights(self):\n",
    "    # to initialize all parameters from normal distribution\n",
    "    # helps with converging during training\n",
    "    for name, param in self.named_parameters():\n",
    "      nn.init.normal_(param.data, mean=0, std=0.1)\n",
    "\n",
    "  def init_embeddings(self, word_pad_idx):\n",
    "    # initialize embedding for padding as zero\n",
    "    self.embedding.weight.data[word_pad_idx] = torch.zeros(self.embedding_dim)\n",
    "\n",
    "  def count_parameters(self):\n",
    "    return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 709,352 trainable parameters.\n",
      "BiLSTM(\n",
      "  (embedding): Embedding(5240, 100, padding_idx=1)\n",
      "  (emb_dropout): Dropout(p=0.5, inplace=False)\n",
      "  (lstm): LSTM(100, 64, num_layers=2, dropout=0.1, bidirectional=True)\n",
      "  (fc_dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc): Linear(in_features=128, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "bilstm = BiLSTM(\n",
    "    input_dim=len(crp.word_field.vocab),\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=64,\n",
    "    output_dim=len(crp.tag_field.vocab),\n",
    "    lstm_layers=2,\n",
    "    emb_dropout=0.5,\n",
    "    lstm_dropout=0.1,\n",
    "    fc_dropout=0.25,\n",
    "    word_pad_idx=crp.word_pad_idx\n",
    ")\n",
    "bilstm.init_weights()\n",
    "bilstm.init_embeddings(word_pad_idx=crp.word_pad_idx)\n",
    "print(f\"The model has {bilstm.count_parameters():,} trainable parameters.\")\n",
    "print(bilstm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class LID(object):\n",
    "\n",
    "    def __init__(self, model, data, optimizer_cls, loss_fn_cls):\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.optimizer = optimizer_cls(model.parameters())\n",
    "        self.loss_fn = loss_fn_cls(ignore_index=self.data.tag_pad_idx)\n",
    "\n",
    "    @staticmethod\n",
    "    def epoch_time(start_time, end_time):\n",
    "        elapsed_time = end_time - start_time\n",
    "        elapsed_mins = int(elapsed_time / 60)\n",
    "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "        return elapsed_mins, elapsed_secs\n",
    "\n",
    "    def accuracy(self, preds, y):\n",
    "        max_preds = preds.argmax(dim=1, keepdim=True)  # get the index of the max probability\n",
    "        non_pad_elements = (y != self.data.tag_pad_idx).nonzero()  # prepare masking for paddings\n",
    "        correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
    "        return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]])\n",
    "\n",
    "    def epoch(self):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        self.model.train()\n",
    "        for batch in self.data.train_iter:\n",
    "            # text = [sent len, batch size]\n",
    "            text = batch.word\n",
    "            # tags = [sent len, batch size]\n",
    "            true_tags = batch.tag\n",
    "            self.optimizer.zero_grad()\n",
    "            pred_tags = self.model(text)\n",
    "            # to calculate the loss and accuracy, we flatten both prediction and true tags\n",
    "            # flatten pred_tags to [sent len, batch size, output dim]\n",
    "            pred_tags = pred_tags.view(-1, pred_tags.shape[-1])\n",
    "            # flatten true_tags to [sent len * batch size]\n",
    "            true_tags = true_tags.view(-1)\n",
    "            batch_loss = self.loss_fn(pred_tags, true_tags)\n",
    "            batch_acc = self.accuracy(pred_tags, true_tags)\n",
    "            batch_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            epoch_loss += batch_loss.item()\n",
    "            epoch_acc += batch_acc.item()\n",
    "\n",
    "        return epoch_loss / len(self.data.train_iter), epoch_acc / len(self.data.train_iter)\n",
    "\n",
    "    def evaluate(self, iterator):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "        # similar to epoch() but model is in evaluation mode and no backprop\n",
    "            for batch in iterator:\n",
    "                text = batch.word\n",
    "                true_tags = batch.tag\n",
    "                pred_tags = self.model(text)\n",
    "                pred_tags = pred_tags.view(-1, pred_tags.shape[-1])\n",
    "                true_tags = true_tags.view(-1)\n",
    "                batch_loss = self.loss_fn(pred_tags, true_tags)\n",
    "                batch_acc = self.accuracy(pred_tags, true_tags)\n",
    "                epoch_loss += batch_loss.item()\n",
    "                epoch_acc += batch_acc.item()\n",
    "\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "    # main training sequence\n",
    "    def train(self, n_epochs):\n",
    "        for epoch in range(n_epochs):\n",
    "            start_time = time.time()\n",
    "            train_loss, train_acc = self.epoch()\n",
    "            end_time = time.time()\n",
    "            epoch_mins, epoch_secs = LID.epoch_time(start_time, end_time)\n",
    "            print(f\"Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "            print(f\"\\tTrn Loss: {train_loss:.3f} | Trn Acc: {train_acc * 100:.2f}%\")\n",
    "            val_loss, val_acc = self.evaluate(self.data.val_iter)\n",
    "            print(f\"\\tVal Loss: {val_loss:.3f} | Val Acc: {val_acc * 100:.2f}%\")\n",
    "        test_loss, test_acc = self.evaluate(self.data.test_iter)\n",
    "        print(f\"Test Loss: {test_loss:.3f} |  Test Acc: {test_acc * 100:.2f}%\")\n",
    "\n",
    "    def infer(self, sentence, true_tags=None):\n",
    "        self.model.eval()\n",
    "        # tokenize sentence\n",
    "        nlp = Indonesian()\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "        # transform to indices based on corpus vocab\n",
    "        numericalized_tokens = [self.data.word_field.vocab.stoi[t] for t in tokens]\n",
    "        # find unknown words\n",
    "        unk_idx = self.data.word_field.vocab.stoi[self.data.word_field.unk_token]\n",
    "        unks = [t for t, n in zip(tokens, numericalized_tokens) if n == unk_idx]\n",
    "        # begin prediction\n",
    "        token_tensor = torch.LongTensor(numericalized_tokens)\n",
    "        token_tensor = token_tensor.unsqueeze(-1)\n",
    "        predictions = self.model(token_tensor)\n",
    "        # convert results to tags\n",
    "        top_predictions = predictions.argmax(-1)\n",
    "        predicted_tags = [self.data.tag_field.vocab.itos[t.item()] for t in top_predictions]\n",
    "        # print inferred tags\n",
    "        max_len_token = max([len(token) for token in tokens] + [len(\"word\")])\n",
    "        max_len_tag = max([len(tag) for tag in predicted_tags] + [len(\"pred\")])\n",
    "        print(\n",
    "            f\"{'word'.ljust(max_len_token)}\\t{'unk'.ljust(max_len_token)}\\t{'pred tag'.ljust(max_len_tag)}\"\n",
    "            + (\"\\ttrue tag\" if true_tags else \"\")\n",
    "            )\n",
    "        for i, token in enumerate(tokens):\n",
    "            is_unk = \"✓\" if token in unks else \"\"\n",
    "            print(\n",
    "              f\"{token.ljust(max_len_token)}\\t{is_unk.ljust(max_len_token)}\\t{predicted_tags[i].ljust(max_len_tag)}\"\n",
    "              + (f\"\\t{true_tags[i]}\" if true_tags else \"\")\n",
    "              )\n",
    "\n",
    "        return tokens, predicted_tags, unks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 7s\n",
      "\tTrn Loss: 1.317 | Trn Acc: 52.65%\n",
      "\tVal Loss: 1.025 | Val Acc: 66.39%\n",
      "Epoch: 02 | Epoch Time: 0m 7s\n",
      "\tTrn Loss: 0.684 | Trn Acc: 76.94%\n",
      "\tVal Loss: 0.588 | Val Acc: 80.06%\n",
      "Epoch: 03 | Epoch Time: 0m 6s\n",
      "\tTrn Loss: 0.460 | Trn Acc: 84.39%\n",
      "\tVal Loss: 0.468 | Val Acc: 83.72%\n",
      "Epoch: 04 | Epoch Time: 0m 7s\n",
      "\tTrn Loss: 0.366 | Trn Acc: 87.55%\n",
      "\tVal Loss: 0.399 | Val Acc: 86.19%\n",
      "Epoch: 05 | Epoch Time: 0m 6s\n",
      "\tTrn Loss: 0.302 | Trn Acc: 89.65%\n",
      "\tVal Loss: 0.352 | Val Acc: 87.98%\n",
      "Epoch: 06 | Epoch Time: 0m 6s\n",
      "\tTrn Loss: 0.263 | Trn Acc: 91.06%\n",
      "\tVal Loss: 0.327 | Val Acc: 88.55%\n",
      "Epoch: 07 | Epoch Time: 0m 6s\n",
      "\tTrn Loss: 0.236 | Trn Acc: 91.89%\n",
      "\tVal Loss: 0.315 | Val Acc: 89.27%\n",
      "Epoch: 08 | Epoch Time: 0m 7s\n",
      "\tTrn Loss: 0.221 | Trn Acc: 92.51%\n",
      "\tVal Loss: 0.314 | Val Acc: 89.45%\n",
      "Epoch: 09 | Epoch Time: 0m 7s\n",
      "\tTrn Loss: 0.208 | Trn Acc: 93.00%\n",
      "\tVal Loss: 0.302 | Val Acc: 89.61%\n",
      "Epoch: 10 | Epoch Time: 0m 6s\n",
      "\tTrn Loss: 0.196 | Trn Acc: 93.30%\n",
      "\tVal Loss: 0.301 | Val Acc: 89.65%\n",
      "Test Loss: 0.317 |  Test Acc: 89.20%\n"
     ]
    }
   ],
   "source": [
    "lid = LID(\n",
    "  model=bilstm,\n",
    "  data=crp,\n",
    "  optimizer_cls=Adam,\n",
    "  loss_fn_cls=nn.CrossEntropyLoss\n",
    ")\n",
    "lid.train(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word   \tunk    \tpred tag \ttrue tag\n",
      "if     \t       \tEN       \tEN\n",
      "i      \t       \tEN       \tEN\n",
      "am     \t       \tEN       \tEN\n",
      "happy  \t       \tEN       \tEN\n",
      ",      \t       \tO        \tO\n",
      "aku    \t       \tID       \tID\n",
      "akan   \t       \tID       \tID\n",
      "ngesave\t       \tMIX-ID-EN\tMIX-ID-EN\n",
      "semua  \t       \tID       \tID\n",
      "aja    \t       \tID       \tID\n",
      "haha   \t       \tO        \tO\n",
      "!      \t       \tO        \tO\n",
      "!      \t       \tO        \tJV\n",
      "kowe   \t       \tJV       \tJV\n",
      "ki     \t       \tJV       \tJV\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-25-4b3337e5afe9>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0msentence\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"If I am happy, aku akan ngesave semua aja haha !! kowe ki piye\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mtags\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m\"EN\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"EN\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"EN\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"EN\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"O\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"ID\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"ID\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"MIX-ID-EN\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"ID\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"ID\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"O\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"O\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"JV\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"JV\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"JV\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mwords\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfer_tags\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0munknown_tokens\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlid\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minfer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentence\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msentence\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrue_tags\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtags\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-19-50d13406b298>\u001B[0m in \u001B[0;36minfer\u001B[0;34m(self, sentence, true_tags)\u001B[0m\n\u001B[1;32m    106\u001B[0m             print(\n\u001B[1;32m    107\u001B[0m               \u001B[0;34mf\"{token.ljust(max_len_token)}\\t{is_unk.ljust(max_len_token)}\\t{predicted_tags[i].ljust(max_len_tag)}\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 108\u001B[0;31m               \u001B[0;34m+\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;34mf\"\\t{true_tags[i]}\"\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mtrue_tags\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m\"\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    109\u001B[0m               )\n\u001B[1;32m    110\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "sentence = \"If I am happy, aku akan ngesave semua aja haha !! kowe ki piye\"\n",
    "tags = [\"EN\",\"EN\",\"EN\",\"EN\",\"O\",\"ID\",\"ID\",\"MIX-ID-EN\",\"ID\",\"ID\",\"O\",\"O\",\"JV\",\"JV\",\"JV\"]\n",
    "words, infer_tags, unknown_tokens = lid.infer(sentence=sentence, true_tags=tags)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}