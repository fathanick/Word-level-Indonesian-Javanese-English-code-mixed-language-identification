{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from helper.splitter import sentence_splitter\n",
    "from langid_crf import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "langid = LanguageIdentifier()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "model = pickle.load(open('model/crf_model_split.pkl', 'rb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5621\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_excel('../raw dataset/data-160722.xlsx')\n",
    "df2 = pd.read_excel('../raw dataset/data-170722.xlsx')\n",
    "df3 = pd.read_excel('../raw dataset/data-170722-2.xlsx')\n",
    "count = len(df1) + len(df2) + len(df3)\n",
    "print(count)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0                                              tweet\n0           0  G tahu kenapa Nemu replay ini, bacanya bernada...\n1           1  lu tau ga si gue mimpi ada bl indonesia viral ...\n2           2  @miss_pun @7btsupdates @BTS_twt Iya ya ampun, ...\n3           3  suka semua gilaakk! swim,slow down, friends it...\n4           4  @lersjend si paling manusia tw. chat di sosmed...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>G tahu kenapa Nemu replay ini, bacanya bernada...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>lu tau ga si gue mimpi ada bl indonesia viral ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>@miss_pun @7btsupdates @BTS_twt Iya ya ampun, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>suka semua gilaakk! swim,slow down, friends it...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>@lersjend si paling manusia tw. chat di sosmed...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0                                              tweet\n0           0  @convomf NGGAKKK wkwkkwwk cuma pernah ngerepos...\n1           1    Hobi belum tdr sibuk ngerepost postingan wkwkwk\n2           2  Gua takut banget ntar j-hope ngerepost 1¬≤ tuh ...\n3           3  menunggu hobi ngerepost postingan teman2nya wk...\n4           4  Anjirr anjirrü´† Tapi terus Bright ngerepost pos...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>@convomf NGGAKKK wkwkkwwk cuma pernah ngerepos...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Hobi belum tdr sibuk ngerepost postingan wkwkwk</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Gua takut banget ntar j-hope ngerepost 1¬≤ tuh ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>menunggu hobi ngerepost postingan teman2nya wk...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Anjirr anjirrü´† Tapi terus Bright ngerepost pos...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat = pd.concat([df1, df2, df3])\n",
    "df_concat.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "0       @convomf NGGAKKK wkwkkwwk cuma pernah ngerepos...\n1         Hobi belum tdr sibuk ngerepost postingan wkwkwk\n2       Gua takut banget ntar j-hope ngerepost 1¬≤ tuh ...\n3       menunggu hobi ngerepost postingan teman2nya wk...\n4       Anjirr anjirrü´† Tapi terus Bright ngerepost pos...\n                              ...                        \n1810    @Sylspyy jgn lupa pipis sebelum mulai ngerjain...\n1811    @tokopedia aku sudah denger, lagunya enak bang...\n1812         @puthutpa Relax, mikir kui mangan enak terus\n1813    tapi at the same time night time is the only t...\n1814    Udah berapa hari ini otak ini dipake mikir ker...\nName: tweet, Length: 5621, dtype: object"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = df_concat.tweet\n",
    "tweets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "all_tkn = []\n",
    "x_dt = []\n",
    "for twt in tweets:\n",
    "    #print(twt)\n",
    "    tkns = sentence_splitter(twt)\n",
    "    all_tkn.append(tkns)\n",
    "    x_dt.append(langid.feature_extraction(tkns))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'token.lower': '@convomf',\n  'n_gram_0': '@convomf',\n  'token_BOS': True,\n  'token_EOS': False,\n  'token.prefix_2': '@c',\n  'token.prefix_3': '@co',\n  'token.suffix_2': 'mf',\n  'token.suffix_3': 'omf',\n  'token.length': 8,\n  'token.is_alpha': False,\n  'token.is_numeric': False,\n  'token.is_capital': False,\n  'token.is_title': False,\n  'token.startswith_symbols': True,\n  'token.contains_numeric': False,\n  'token.contains_capital': False,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': 'nggakkk',\n  'n_gram_0': 'NGGAKKK',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': 'NG',\n  'token.prefix_3': 'NGG',\n  'token.suffix_2': 'KK',\n  'token.suffix_3': 'KKK',\n  'token.length': 7,\n  'token.is_alpha': True,\n  'token.is_numeric': False,\n  'token.is_capital': True,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': True,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': 'wkwkkwwk',\n  'n_gram_0': 'wkwkkwwk',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': 'wk',\n  'token.prefix_3': 'wkw',\n  'token.suffix_2': 'wk',\n  'token.suffix_3': 'wwk',\n  'token.length': 8,\n  'token.is_alpha': True,\n  'token.is_numeric': False,\n  'token.is_capital': False,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': False,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': 'cuma',\n  'n_gram_0': 'cuma',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': 'cu',\n  'token.prefix_3': 'cum',\n  'token.suffix_2': 'ma',\n  'token.suffix_3': 'uma',\n  'token.length': 4,\n  'token.is_alpha': True,\n  'token.is_numeric': False,\n  'token.is_capital': False,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': False,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': 'pernah',\n  'n_gram_0': 'pernah',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': 'pe',\n  'token.prefix_3': 'per',\n  'token.suffix_2': 'ah',\n  'token.suffix_3': 'nah',\n  'token.length': 6,\n  'token.is_alpha': True,\n  'token.is_numeric': False,\n  'token.is_capital': False,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': False,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': 'ngerepost',\n  'n_gram_0': 'ngerepost',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': 'ng',\n  'token.prefix_3': 'nge',\n  'token.suffix_2': 'st',\n  'token.suffix_3': 'ost',\n  'token.length': 9,\n  'token.is_alpha': True,\n  'token.is_numeric': False,\n  'token.is_capital': False,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': False,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': 'postingan',\n  'n_gram_0': 'postingan',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': 'po',\n  'token.prefix_3': 'pos',\n  'token.suffix_2': 'an',\n  'token.suffix_3': 'gan',\n  'token.length': 9,\n  'token.is_alpha': True,\n  'token.is_numeric': False,\n  'token.is_capital': False,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': False,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': 'biasku',\n  'n_gram_0': 'biasku',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': 'bi',\n  'token.prefix_3': 'bia',\n  'token.suffix_2': 'ku',\n  'token.suffix_3': 'sku',\n  'token.length': 6,\n  'token.is_alpha': True,\n  'token.is_numeric': False,\n  'token.is_capital': False,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': False,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': 'satu',\n  'n_gram_0': 'SATU',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': 'SA',\n  'token.prefix_3': 'SAT',\n  'token.suffix_2': 'TU',\n  'token.suffix_3': 'ATU',\n  'token.length': 4,\n  'token.is_alpha': True,\n  'token.is_numeric': False,\n  'token.is_capital': True,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': True,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': 'kaliii',\n  'n_gram_0': 'KALIII',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': 'KA',\n  'token.prefix_3': 'KAL',\n  'token.suffix_2': 'II',\n  'token.suffix_3': 'III',\n  'token.length': 6,\n  'token.is_alpha': True,\n  'token.is_numeric': False,\n  'token.is_capital': True,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': True,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': 'doanggg',\n  'n_gram_0': 'DOANGGG',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': 'DO',\n  'token.prefix_3': 'DOA',\n  'token.suffix_2': 'GG',\n  'token.suffix_3': 'GGG',\n  'token.length': 7,\n  'token.is_alpha': True,\n  'token.is_numeric': False,\n  'token.is_capital': True,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': True,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': 'wkwkwkwk',\n  'n_gram_0': 'wkwkwkwk',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': 'wk',\n  'token.prefix_3': 'wkw',\n  'token.suffix_2': 'wk',\n  'token.suffix_3': 'kwk',\n  'token.length': 8,\n  'token.is_alpha': True,\n  'token.is_numeric': False,\n  'token.is_capital': False,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': False,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': 'oh',\n  'n_gram_0': 'oh',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': 'oh',\n  'token.prefix_3': 'oh',\n  'token.suffix_2': 'oh',\n  'token.suffix_3': 'oh',\n  'token.length': 2,\n  'token.is_alpha': True,\n  'token.is_numeric': False,\n  'token.is_capital': False,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': False,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': 'sama',\n  'n_gram_0': 'sama',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': 'sa',\n  'token.prefix_3': 'sam',\n  'token.suffix_2': 'ma',\n  'token.suffix_3': 'ama',\n  'token.length': 4,\n  'token.is_alpha': True,\n  'token.is_numeric': False,\n  'token.is_capital': False,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': False,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': 'bikin',\n  'n_gram_0': 'bikin',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': 'bi',\n  'token.prefix_3': 'bik',\n  'token.suffix_2': 'in',\n  'token.suffix_3': 'kin',\n  'token.length': 5,\n  'token.is_alpha': True,\n  'token.is_numeric': False,\n  'token.is_capital': False,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': False,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': 'story',\n  'n_gram_0': 'story',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': 'st',\n  'token.prefix_3': 'sto',\n  'token.suffix_2': 'ry',\n  'token.suffix_3': 'ory',\n  'token.length': 5,\n  'token.is_alpha': True,\n  'token.is_numeric': False,\n  'token.is_capital': False,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': False,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': 'lagu',\n  'n_gram_0': 'lagu',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': 'la',\n  'token.prefix_3': 'lag',\n  'token.suffix_2': 'gu',\n  'token.suffix_3': 'agu',\n  'token.length': 4,\n  'token.is_alpha': True,\n  'token.is_numeric': False,\n  'token.is_capital': False,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': False,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': 'sekali',\n  'n_gram_0': 'sekali',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': 'se',\n  'token.prefix_3': 'sek',\n  'token.suffix_2': 'li',\n  'token.suffix_3': 'ali',\n  'token.length': 6,\n  'token.is_alpha': True,\n  'token.is_numeric': False,\n  'token.is_capital': False,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': False,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': 'juga',\n  'n_gram_0': 'juga',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': 'ju',\n  'token.prefix_3': 'jug',\n  'token.suffix_2': 'ga',\n  'token.suffix_3': 'uga',\n  'token.length': 4,\n  'token.is_alpha': True,\n  'token.is_numeric': False,\n  'token.is_capital': False,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': False,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': ',',\n  'n_gram_0': ',',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': ',',\n  'token.prefix_3': ',',\n  'token.suffix_2': ',',\n  'token.suffix_3': ',',\n  'token.length': 1,\n  'token.is_alpha': False,\n  'token.is_numeric': False,\n  'token.is_capital': False,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': False,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': 'jadi',\n  'n_gram_0': 'jadi',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': 'ja',\n  'token.prefix_3': 'jad',\n  'token.suffix_2': 'di',\n  'token.suffix_3': 'adi',\n  'token.length': 4,\n  'token.is_alpha': True,\n  'token.is_numeric': False,\n  'token.is_capital': False,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': False,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': 'baru',\n  'n_gram_0': 'baru',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': 'ba',\n  'token.prefix_3': 'bar',\n  'token.suffix_2': 'ru',\n  'token.suffix_3': 'aru',\n  'token.length': 4,\n  'token.is_alpha': True,\n  'token.is_numeric': False,\n  'token.is_capital': False,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': False,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': '2x',\n  'n_gram_0': '2x',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': '2x',\n  'token.prefix_3': '2x',\n  'token.suffix_2': '2x',\n  'token.suffix_3': '2x',\n  'token.length': 2,\n  'token.is_alpha': False,\n  'token.is_numeric': False,\n  'token.is_capital': False,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': True,\n  'token.contains_capital': False,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': 'di',\n  'n_gram_0': 'di',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': 'di',\n  'token.prefix_3': 'di',\n  'token.suffix_2': 'di',\n  'token.suffix_3': 'di',\n  'token.length': 2,\n  'token.is_alpha': True,\n  'token.is_numeric': False,\n  'token.is_capital': False,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': False,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': 'akun',\n  'n_gram_0': 'akun',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': 'ak',\n  'token.prefix_3': 'aku',\n  'token.suffix_2': 'un',\n  'token.suffix_3': 'kun',\n  'token.length': 4,\n  'token.is_alpha': True,\n  'token.is_numeric': False,\n  'token.is_capital': False,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': False,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': 'pertama',\n  'n_gram_0': 'pertama',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': 'pe',\n  'token.prefix_3': 'per',\n  'token.suffix_2': 'ma',\n  'token.suffix_3': 'ama',\n  'token.length': 7,\n  'token.is_alpha': True,\n  'token.is_numeric': False,\n  'token.is_capital': False,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': False,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': ',',\n  'n_gram_0': ',',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': ',',\n  'token.prefix_3': ',',\n  'token.suffix_2': ',',\n  'token.suffix_3': ',',\n  'token.length': 1,\n  'token.is_alpha': False,\n  'token.is_numeric': False,\n  'token.is_capital': False,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': False,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': 'kalo',\n  'n_gram_0': 'kalo',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': 'ka',\n  'token.prefix_3': 'kal',\n  'token.suffix_2': 'lo',\n  'token.suffix_3': 'alo',\n  'token.length': 4,\n  'token.is_alpha': True,\n  'token.is_numeric': False,\n  'token.is_capital': False,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': False,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': 'diakun',\n  'n_gram_0': 'diakun',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': 'di',\n  'token.prefix_3': 'dia',\n  'token.suffix_2': 'un',\n  'token.suffix_3': 'kun',\n  'token.length': 6,\n  'token.is_alpha': True,\n  'token.is_numeric': False,\n  'token.is_capital': False,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': False,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': 'kedua',\n  'n_gram_0': 'kedua',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': 'ke',\n  'token.prefix_3': 'ked',\n  'token.suffix_2': 'ua',\n  'token.suffix_3': 'dua',\n  'token.length': 5,\n  'token.is_alpha': True,\n  'token.is_numeric': False,\n  'token.is_capital': False,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': False,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': 'mah',\n  'n_gram_0': 'mah',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': 'ma',\n  'token.prefix_3': 'mah',\n  'token.suffix_2': 'ah',\n  'token.suffix_3': 'mah',\n  'token.length': 3,\n  'token.is_alpha': True,\n  'token.is_numeric': False,\n  'token.is_capital': False,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': False,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': 'tiap',\n  'n_gram_0': 'tiap',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': 'ti',\n  'token.prefix_3': 'tia',\n  'token.suffix_2': 'ap',\n  'token.suffix_3': 'iap',\n  'token.length': 4,\n  'token.is_alpha': True,\n  'token.is_numeric': False,\n  'token.is_capital': False,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': False,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': 'hari',\n  'n_gram_0': 'hari',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': 'ha',\n  'token.prefix_3': 'har',\n  'token.suffix_2': 'ri',\n  'token.suffix_3': 'ari',\n  'token.length': 4,\n  'token.is_alpha': True,\n  'token.is_numeric': False,\n  'token.is_capital': False,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': False,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': 'wkwkkww',\n  'n_gram_0': 'wkwkkww',\n  'token_BOS': False,\n  'token_EOS': False,\n  'token.prefix_2': 'wk',\n  'token.prefix_3': 'wkw',\n  'token.suffix_2': 'ww',\n  'token.suffix_3': 'kww',\n  'token.length': 7,\n  'token.is_alpha': True,\n  'token.is_numeric': False,\n  'token.is_capital': False,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': False,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False},\n {'token.lower': 'üëç',\n  'n_gram_0': 'üëç',\n  'token_BOS': False,\n  'token_EOS': True,\n  'token.prefix_2': 'üëç',\n  'token.prefix_3': 'üëç',\n  'token.suffix_2': 'üëç',\n  'token.suffix_3': 'üëç',\n  'token.length': 1,\n  'token.is_alpha': False,\n  'token.is_numeric': False,\n  'token.is_capital': False,\n  'token.is_title': False,\n  'token.startswith_symbols': False,\n  'token.contains_numeric': False,\n  'token.contains_capital': False,\n  'token.contains_quotes': False,\n  'token.contains_hyphen': False}]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_dt[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "tag_pred = model.predict(x_dt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "target_file = '../raw dataset/ijelid-tagged-170722.tsv'\n",
    "with open(target_file, 'a', encoding='utf-8') as f:\n",
    "    tkn_tag = [[tok, tg] for tok, tg in zip(all_tkn, tag_pred)]\n",
    "    #print(tkn_tag[0])\n",
    "    for tk, tg in tkn_tag:\n",
    "        lst = list(zip(tk,tg))\n",
    "        for l in lst:\n",
    "            f.write('\\t'.join([str(i) for i in l]))\n",
    "            f.write('\\n')\n",
    "        f.write('\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "df_concat.to_excel('../raw dataset/dataset-merge-5621.xlsx')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}